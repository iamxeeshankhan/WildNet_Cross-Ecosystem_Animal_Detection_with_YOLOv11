{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM3sKv3c9L0Ap0FcEDOsgfF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install ultralytics===8.3.213\n","from google.colab import files\n","import yaml\n","from ultralytics import YOLO"],"metadata":{"id":"3Vn0qaHMj_ly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Download & unzip the requried Datasets**"],"metadata":{"id":"kdNcXXUff4hl"}},{"cell_type":"code","source":["!wget \"https://www.kaggle.com/api/v1/datasets/download/izeeshan/unified-animal-species-dataset\""],"metadata":{"id":"3Caj7zSfLdL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p /content/unified_animal_species_dataset && unzip /content/unified-animal-species-dataset -d /content/unified_animal_species_dataset && rm /content/unified-animal-species-dataset"],"metadata":{"id":"Rte9Rp7JLgHU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set the path to /content/unified_animal_species_dataset in the dataset.yaml\n","\n","yaml_path = '/content/unified_animal_species_dataset/dataset.yaml'\n","\n","with open(yaml_path, 'r') as f:\n","    data = yaml.safe_load(f)\n","\n","data['path'] = '/content/unified_animal_species_dataset'\n","\n","with open(yaml_path, 'w') as f:\n","    yaml.safe_dump(data, f, sort_keys=False)\n"],"metadata":{"id":"7PBn95CqhGcL","executionInfo":{"status":"ok","timestamp":1760845333436,"user_tz":-300,"elapsed":65,"user":{"displayName":"M. Zeeshan Khan","userId":"13618057998455684979"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# **Train the Model**"],"metadata":{"id":"VuCfiouZmdc4"}},{"cell_type":"code","source":["# Load YOLOv11 model\n","model = YOLO('yolo11m.pt')\n","\n","# Train the model\n","results = model.train(\n","    data=yaml_path,\n","    epochs=2,\n","    imgsz=640,\n","    batch=16\n",")\n","\n","\"\"\"\n","'results' might include:\n","  results.metrics: final training metrics (mAP, precision, recall, etc.)\n","  results.save_dir: directory path where all training outputs are stored\n","  results.model: reference to the trained model\n","  results.epoch: number of completed epochs\n","\"\"\""],"metadata":{"id":"iCRJN9g2QHfP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Evaluate the model**"],"metadata":{"id":"FoaE2UsaoBqA"}},{"cell_type":"code","source":["model = YOLO(\"/content/yolo_dataset/runs/detect/train/weights/best.pt\")\n","\n","# Validate on your dataset (this gives precision, recall, mAP, confusion matrix, etc.)\n","metrics = model.val(\n","    data=\"/content/dataset/meta.yaml\",\n","    split=\"test\",\n","    imgsz=512\n",")\n","\n","print(metrics)  # shows metrics dict"],"metadata":{"id":"NaYXZgC0oA6Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Inference**"],"metadata":{"id":"xU1Wg0-rrP_d"}},{"cell_type":"code","source":["# Load your trained model\n","model = YOLO(\"/content/yolo_dataset/runs/detect/train/weights/best.pt\")\n","\n","# Upload an image\n","uploaded = files.upload()\n","\n","# Run inference (object detection)\n","for filename in uploaded.keys():\n","    results = model.predict(\n","        source=filename,\n","        save=True, # save annotated image(s)\n","        conf=0.25, # confidence threshold\n","        imgsz=640\n","    )\n","\n","    print(f\"Processed {filename}\")\n"],"metadata":{"id":"HHwuxRW4rPLS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Optional Step - Exporting the Trained Model (for Deployment & Portability)**"],"metadata":{"id":"c2gUtVMTmZxs"}},{"cell_type":"code","source":["# PyTorch format (.pt)\n","# Export converts it to other formats:\n","# ONNX → open format, runs on many platforms (good for production).\n","# TorchScript → optimized PyTorch model (faster inference).\n","# TensorRT / CoreML → for NVIDIA GPUs / Apple devices."],"metadata":{"id":"drmVXLIZi7Je"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# IoU (Intersection over union) = how much the modal drawn box overlapped with the real box (ground truth).\n","# mAP = your average score across all objects and overlap thresholds.\n","# Export = translating your model into another “language” so other systems can use it."],"metadata":{"id":"iDmdJfwPm8BI"},"execution_count":null,"outputs":[]}]}