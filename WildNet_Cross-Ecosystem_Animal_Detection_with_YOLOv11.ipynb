{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdNcXXUff4hl"
   },
   "source": [
    "# **Download the requried Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmKHrirOLKEx"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "def download_kaggle_dataset(url, output):\n",
    "    try:\n",
    "        # stream request\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        total_size = int(response.headers.get(\"content-length\", 0))\n",
    "        block_size = 1024  # 1 KB\n",
    "\n",
    "        with open(output, \"wb\") as file, tqdm(\n",
    "            total=total_size, unit=\"iB\", unit_scale=True, desc=f\"Downloading -> {output}\"\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                bar.update(len(data))\n",
    "                file.write(data)\n",
    "\n",
    "        print(f\"‚úÖ Downloaded to {output}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to download {url}: {e}\")\n",
    "\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "\n",
    "# list of datasets\n",
    "# find a kaggle dataset, click on download -> cURL -> copy the URL of dataset only\n",
    "datasets_list = [\n",
    "    {\"url\": \" https://www.kaggle.com/api/v1/datasets/download/iamsouravbanerjee/animal-image-dataset-90-different-animals\",\n",
    "     \"output\": \"/content/datasets/90-mix-animals.zip\",\n",
    "     \"category\": \"mix\"},\n",
    "\n",
    "    {\"url\": \"https://www.kaggle.com/api/v1/datasets/download/antoreepjana/animals-detection-images-dataset\",\n",
    "     \"output\": \"/content/datasets/animals-detection-images-dataset.zip\",\n",
    "     \"category\": \"mix\"},\n",
    "\n",
    "    {\"url\": \"https://www.kaggle.com/api/v1/datasets/download/sharansmenon/aquarium-dataset\",\n",
    "     \"output\": \"/content/datasets/aquarium-dataset.zip\",\n",
    "     \"category\": \"sea\"},\n",
    "\n",
    "    {\"url\": \"https://www.kaggle.com/api/v1/datasets/download/wenewone/cub2002011\",\n",
    "     \"output\": \"/content/datasets/cub2002011.zip\",\n",
    "     \"category\": \"air\"},\n",
    "\n",
    "    {\"url\": \"https://www.kaggle.com/api/v1/datasets/download/duyminhle/nabirds\",\n",
    "     \"output\": \"/content/datasets/nabirds.zip\",\n",
    "     \"category\": \"air\"},\n",
    "\n",
    "    {\"url\": \"https://www.kaggle.com/api/v1/datasets/download/alessiocorrado99/animals10\",\n",
    "     \"output\": \"/content/datasets/animals10.zip\",\n",
    "     \"category\": \"land\"},\n",
    "]\n",
    "\n",
    "# Create the 'dataset' directory\n",
    "os.makedirs(\"/content/datasets\", exist_ok=True)\n",
    "\n",
    "# loop over datasets\n",
    "for dataset in datasets_list:\n",
    "    try:\n",
    "        download_kaggle_dataset(dataset[\"url\"], dataset[\"output\"])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to download {dataset['url']}: {e}\")\n",
    "\n",
    "    # random delay between 5‚Äì15 seconds\n",
    "    delay = random.uniform(5, 15)\n",
    "    print(f\"‚è≥ Waiting {delay:.2f} seconds before next download...\")\n",
    "    time.sleep(delay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKPtbaUQgCyj"
   },
   "source": [
    "# **Unzip into categorical paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1j1u4sWBbmn"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip_datasets(datasets_list):\n",
    "\n",
    "    for dataset in datasets_list:\n",
    "        input_path = dataset[\"input_path\"]\n",
    "        output_path = dataset[\"output_path\"]\n",
    "\n",
    "        try:\n",
    "            # make sure output directory exists\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "            print(f\"Unzipping {input_path} -> {output_path} ...\")\n",
    "            with zipfile.ZipFile(input_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(output_path)\n",
    "\n",
    "            print(f\"‚úÖ Done: {input_path}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to unzip {input_path}: {e}\\n\")\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "\n",
    "datasets_to_unzip = [\n",
    "    {\"input_path\": \"/content/datasets/90-mix-animals.zip\",\n",
    "     \"output_path\": \"/content/data/mix/90-mix-animals\"},\n",
    "\n",
    "    {\"input_path\": \"/content/datasets/animals-detection-images-dataset.zip\",\n",
    "     \"output_path\": \"/content/data/mix/animals-detection-images-dataset\"},\n",
    "\n",
    "    {\"input_path\": \"/content/datasets/animals10.zip\",\n",
    "     \"output_path\": \"/content/data/land/animals10\"},\n",
    "\n",
    "    {\"input_path\": \"/content/datasets/aquarium-dataset.zip\",\n",
    "     \"output_path\": \"/content/data/sea/aquarium-dataset\"},\n",
    "\n",
    "    {\"input_path\": \"/content/datasets/cub2002011.zip\",\n",
    "     \"output_path\": \"/content/data/air/cub2002011\"},\n",
    "\n",
    "    {\"input_path\": \"/content/datasets/nabirds.zip\",\n",
    "     \"output_path\": \"/content/data/air/nabirds\"}\n",
    "]\n",
    "\n",
    "unzip_datasets(datasets_to_unzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1029,
     "status": "ok",
     "timestamp": 1758946957082,
     "user": {
      "displayName": "M. Zeeshan Khan",
      "userId": "13618057998455684979"
     },
     "user_tz": -300
    },
    "id": "ygXT55TDaFuN"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0xC0PKJgNmv"
   },
   "source": [
    "# **Prepare the Datasets for YOLOv11**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPWMTFaNg5wn"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q ultralytics opencv-python pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1K8XOQ48rVSTz1cibZjR6yCBvmPVvZcjV"
    },
    "executionInfo": {
     "elapsed": 593007,
     "status": "ok",
     "timestamp": 1758947673771,
     "user": {
      "displayName": "M. Zeeshan Khan",
      "userId": "13618057998455684979"
     },
     "user_tz": -300
    },
    "id": "HOSR90cwgSbq",
    "outputId": "6330c216-8499-41d4-9acc-55405b5e184a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare multiple animal datasets for YOLOv11 training\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import cv2\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_yolo_dataset(base_path=\"/content/data\", output_path=\"/content/yolo_dataset\",\n",
    "                        train_split=0.8, val_split=0.1, test_split=0.1,\n",
    "                        max_samples_per_class=None):\n",
    "    \"\"\"\n",
    "    base_path (str): Path to the base data directory\n",
    "    output_path (str): Path where organized YOLO dataset will be saved\n",
    "    train_split (float): Proportion of data for training\n",
    "    val_split (float): Proportion of data for validation\n",
    "    test_split (float): Proportion of data for testing\n",
    "    max_samples_per_class (int): Maximum samples per class (for balancing)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create output directory structure\n",
    "    output_path = Path(output_path)\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        (output_path / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "        (output_path / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Class mapping - will be built dynamically\n",
    "    class_names = set()\n",
    "    class_to_id = {}\n",
    "    all_data = []\n",
    "    dataset_stats = {}\n",
    "\n",
    "    print(\"üîç Processing datasets...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Process CUB-200-2011 (Birds dataset)\n",
    "    cub_path = Path(base_path) / \"air\" / \"cub2002011\" / \"CUB_200_2011\"\n",
    "    if cub_path.exists():\n",
    "        print(\"ü¶Ö Processing CUB-200-2011 dataset...\")\n",
    "        cub_data = process_cub_dataset(cub_path, class_names)\n",
    "        all_data.extend(cub_data)\n",
    "        dataset_stats['CUB-200-2011'] = len(cub_data)\n",
    "        print(f\"   ‚úÖ Found {len(cub_data)} bird samples\")\n",
    "    else:\n",
    "        print(\"‚ùå CUB-200-2011 dataset not found\")\n",
    "\n",
    "    # Process NABirds dataset\n",
    "    nabirds_path = Path(base_path) / \"air\" / \"nabirds\"\n",
    "    if nabirds_path.exists():\n",
    "        print(\"üê¶ Processing NABirds dataset...\")\n",
    "        nabirds_data = process_nabirds_dataset(nabirds_path, class_names)\n",
    "        all_data.extend(nabirds_data)\n",
    "        dataset_stats['NABirds'] = len(nabirds_data)\n",
    "        print(f\"   ‚úÖ Found {len(nabirds_data)} bird samples\")\n",
    "    else:\n",
    "        print(\"‚ùå NABirds dataset not found\")\n",
    "\n",
    "    # Process Animals10 dataset\n",
    "    animals10_path = Path(base_path) / \"land\" / \"animals10\"\n",
    "    if animals10_path.exists():\n",
    "        print(\"üêæ Processing Animals10 dataset...\")\n",
    "        animals10_data = process_animals10_dataset(animals10_path, class_names)\n",
    "        all_data.extend(animals10_data)\n",
    "        dataset_stats['Animals10'] = len(animals10_data)\n",
    "        print(f\"   ‚úÖ Found {len(animals10_data)} land animal samples\")\n",
    "    else:\n",
    "        print(\"‚ùå Animals10 dataset not found\")\n",
    "\n",
    "    # Process 90-mix-animals dataset\n",
    "    mix_animals_path = Path(base_path) / \"mix\" / \"90-mix-animals\"\n",
    "    if mix_animals_path.exists():\n",
    "        print(\"ü¶ì Processing 90-mix-animals dataset...\")\n",
    "        mix_data = process_mix_animals_dataset(mix_animals_path, class_names)\n",
    "        all_data.extend(mix_data)\n",
    "        dataset_stats['90-mix-animals'] = len(mix_data)\n",
    "        print(f\"   ‚úÖ Found {len(mix_data)} mixed animal samples\")\n",
    "    else:\n",
    "        print(\"‚ùå 90-mix-animals dataset not found\")\n",
    "\n",
    "    # Process animals-detection-images dataset (already has YOLO format)\n",
    "    detection_path = Path(base_path) / \"mix\" / \"animals-detection-images-dataset\"\n",
    "    if detection_path.exists():\n",
    "        print(\"üéØ Processing animals-detection-images dataset...\")\n",
    "        detection_data = process_detection_dataset(detection_path, class_names)\n",
    "        all_data.extend(detection_data)\n",
    "        dataset_stats['Animals-Detection'] = len(detection_data)\n",
    "        print(f\"   ‚úÖ Found {len(detection_data)} detection samples\")\n",
    "    else:\n",
    "        print(\"‚ùå Animals-detection-images dataset not found\")\n",
    "\n",
    "    # Process Aquarium dataset\n",
    "    aquarium_path = Path(base_path) / \"sea\" / \"aquarium-dataset\" / \"Aquarium Combined\"\n",
    "    if aquarium_path.exists():\n",
    "        print(\"üê† Processing Aquarium dataset...\")\n",
    "        aquarium_data = process_aquarium_dataset(aquarium_path, class_names)\n",
    "        all_data.extend(aquarium_data)\n",
    "        dataset_stats['Aquarium'] = len(aquarium_data)\n",
    "        print(f\"   ‚úÖ Found {len(aquarium_data)} aquarium samples\")\n",
    "    else:\n",
    "        print(\"‚ùå Aquarium dataset not found\")\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Balance dataset if needed\n",
    "    if max_samples_per_class:\n",
    "        all_data = balance_dataset(all_data, max_samples_per_class)\n",
    "\n",
    "    # Create class mapping with all species\n",
    "    sorted_classes = sorted(list(class_names))\n",
    "    class_to_id = {class_name: idx for idx, class_name in enumerate(sorted_classes)}\n",
    "\n",
    "    # Display dataset statistics\n",
    "    display_dataset_stats(dataset_stats, sorted_classes, all_data)\n",
    "\n",
    "    # Shuffle and split data\n",
    "    random.shuffle(all_data)\n",
    "    train_end = int(len(all_data) * train_split)\n",
    "    val_end = train_end + int(len(all_data) * val_split)\n",
    "\n",
    "    splits = {\n",
    "        'train': all_data[:train_end],\n",
    "        'val': all_data[train_end:val_end],\n",
    "        'test': all_data[val_end:]\n",
    "    }\n",
    "\n",
    "    # Copy files and create labels\n",
    "    for split_name, split_data in splits.items():\n",
    "        print(f\"üìÅ Processing {split_name} split: {len(split_data)} samples\")\n",
    "\n",
    "        for idx, (img_path, class_name, bbox) in enumerate(split_data):\n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Copy image\n",
    "                img_ext = Path(img_path).suffix\n",
    "                new_img_name = f\"{split_name}_{idx:06d}{img_ext}\"\n",
    "                new_img_path = output_path / split_name / 'images' / new_img_name\n",
    "                shutil.copy2(img_path, new_img_path)\n",
    "\n",
    "                # Create YOLO label file\n",
    "                label_path = output_path / split_name / 'labels' / f\"{split_name}_{idx:06d}.txt\"\n",
    "\n",
    "                with open(label_path, 'w') as f:\n",
    "                    class_id = class_to_id[class_name]\n",
    "\n",
    "                    # If bbox is provided, use it; otherwise create full image bbox\n",
    "                    if bbox:\n",
    "                        x_center, y_center, width, height = bbox\n",
    "                        f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "                    else:\n",
    "                        # Full image annotation (for classification datasets)\n",
    "                        f.write(f\"{class_id} 0.5 0.5 1.0 1.0\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error processing {img_path}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    # Create dataset.yaml file with ALL species classes\n",
    "    create_dataset_yaml(output_path, sorted_classes, dataset_stats)\n",
    "\n",
    "    # Display completion summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéâ DATASET PREPARATION COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìä Total Classes: {len(sorted_classes)}\")\n",
    "    print(f\"üìÅ Output Directory: {output_path}\")\n",
    "    print(f\"üöÇ Train: {len(splits['train'])}\")\n",
    "    print(f\"‚úÖ Val: {len(splits['val'])}\")\n",
    "    print(f\"üß™ Test: {len(splits['test'])}\")\n",
    "    print(f\"üìÑ Config File: {output_path}/dataset.yaml\")\n",
    "\n",
    "    return output_path, sorted_classes, splits\n",
    "\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "def display_dataset_stats(dataset_stats, sorted_classes, all_data):\n",
    "    \"\"\"Display comprehensive dataset statistics\"\"\"\n",
    "    print(f\"üìä DATASET STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Dataset breakdown\n",
    "    total_samples = sum(dataset_stats.values())\n",
    "    for dataset_name, count in dataset_stats.items():\n",
    "        percentage = (count / total_samples) * 100\n",
    "        print(f\"{dataset_name:20}: {count:6d} samples ({percentage:5.1f}%)\")\n",
    "\n",
    "    print(f\"{'TOTAL':20}: {total_samples:6d} samples\")\n",
    "    print(f\"\\nüè∑Ô∏è  Total Classes: {len(sorted_classes)}\")\n",
    "\n",
    "    # Show class distribution\n",
    "    class_counts = defaultdict(int)\n",
    "    for _, class_name, _ in all_data:\n",
    "        class_counts[class_name] += 1\n",
    "\n",
    "    print(f\"\\nüìà Top 10 Classes by Sample Count:\")\n",
    "    sorted_classes_by_count = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i, (class_name, count) in enumerate(sorted_classes_by_count[:10]):\n",
    "        print(f\"  {i+1:2d}. {class_name:25}: {count:4d} samples\")\n",
    "\n",
    "    if len(sorted_classes_by_count) > 10:\n",
    "        print(f\"  ... and {len(sorted_classes_by_count) - 10} more classes\")\n",
    "\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "def balance_dataset(all_data, max_samples_per_class):\n",
    "    \"\"\"Balance dataset by limiting samples per class\"\"\"\n",
    "    class_data = defaultdict(list)\n",
    "    for item in all_data:\n",
    "        class_data[item[1]].append(item)\n",
    "\n",
    "    balanced_data = []\n",
    "    for class_name, items in class_data.items():\n",
    "        if len(items) > max_samples_per_class:\n",
    "            items = random.sample(items, max_samples_per_class)\n",
    "        balanced_data.extend(items)\n",
    "\n",
    "    print(f\"‚öñÔ∏è  Dataset balanced: {len(all_data)} -> {len(balanced_data)} samples\")\n",
    "    return balanced_data\n",
    "\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "def process_cub_dataset(cub_path, class_names):\n",
    "    \"\"\"Process CUB-200-2011 dataset with detailed bird species\"\"\"\n",
    "    data = []\n",
    "    images_path = cub_path / \"images\"\n",
    "\n",
    "    # Read classes to get proper species names\n",
    "    classes_file = cub_path / \"classes.txt\"\n",
    "    class_id_to_name = {}\n",
    "    if classes_file.exists():\n",
    "        with open(classes_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    class_id = parts[0]\n",
    "                    class_name = ' '.join(parts[1:]).replace(' ', '_').replace('.', '').replace('-', '_')\n",
    "                    class_id_to_name[class_id] = class_name\n",
    "\n",
    "    # Read bounding boxes\n",
    "    bbox_file = cub_path / \"bounding_boxes.txt\"\n",
    "    bboxes = {}\n",
    "    if bbox_file.exists():\n",
    "        with open(bbox_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    img_id = parts[0]\n",
    "                    x, y, w, h = map(float, parts[1:5])\n",
    "                    bboxes[img_id] = (x, y, w, h)\n",
    "\n",
    "    # Read image info and class labels\n",
    "    images_file = cub_path / \"images.txt\"\n",
    "    labels_file = cub_path / \"image_class_labels.txt\"\n",
    "\n",
    "    img_to_class = {}\n",
    "    if labels_file.exists():\n",
    "        with open(labels_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    img_id = parts[0]\n",
    "                    class_id = parts[1]\n",
    "                    img_to_class[img_id] = class_id\n",
    "\n",
    "    if images_file.exists():\n",
    "        with open(images_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    img_id = parts[0]\n",
    "                    img_path = parts[1]\n",
    "\n",
    "                    full_img_path = images_path / img_path\n",
    "                    if full_img_path.exists() and img_id in img_to_class:\n",
    "                        class_id = img_to_class[img_id]\n",
    "                        if class_id in class_id_to_name:\n",
    "                            class_name = class_id_to_name[class_id]\n",
    "                        else:\n",
    "                            # Fallback to folder name\n",
    "                            class_name = img_path.split('/')[0].split('.', 1)[1] if '.' in img_path.split('/')[0] else img_path.split('/')[0]\n",
    "                            class_name = class_name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "                        class_names.add(class_name)\n",
    "\n",
    "                        # Convert bbox to YOLO format if available\n",
    "                        bbox = None\n",
    "                        if img_id in bboxes:\n",
    "                            try:\n",
    "                                img = cv2.imread(str(full_img_path))\n",
    "                                if img is not None:\n",
    "                                    img_h, img_w = img.shape[:2]\n",
    "                                    x, y, w, h = bboxes[img_id]\n",
    "                                    # Convert to YOLO format\n",
    "                                    x_center = (x + w/2) / img_w\n",
    "                                    y_center = (y + h/2) / img_h\n",
    "                                    w_norm = w / img_w\n",
    "                                    h_norm = h / img_h\n",
    "                                    bbox = (x_center, y_center, w_norm, h_norm)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                        data.append((str(full_img_path), class_name, bbox))\n",
    "\n",
    "    return data\n",
    "\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "def process_nabirds_dataset(nabirds_path, class_names):\n",
    "    \"\"\"Process NABirds dataset with detailed species names\"\"\"\n",
    "    data = []\n",
    "    images_path = nabirds_path / \"images\"\n",
    "\n",
    "    # Read class names\n",
    "    classes_file = nabirds_path / \"classes.txt\"\n",
    "    class_id_to_name = {}\n",
    "    if classes_file.exists():\n",
    "        with open(classes_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    class_id = parts[0]\n",
    "                    class_name = ' '.join(parts[1:]).replace(' ', '_').replace('-', '_')\n",
    "                    class_id_to_name[class_id] = class_name\n",
    "\n",
    "    # Read images and their classes\n",
    "    images_file = nabirds_path / \"images.txt\"\n",
    "    labels_file = nabirds_path / \"image_class_labels.txt\"\n",
    "\n",
    "    img_to_class = {}\n",
    "    if labels_file.exists():\n",
    "        with open(labels_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    img_id = parts[0]\n",
    "                    class_id = parts[1]\n",
    "                    img_to_class[img_id] = class_id\n",
    "\n",
    "    img_id_to_path = {}\n",
    "    if images_file.exists():\n",
    "        with open(images_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    img_id = parts[0]\n",
    "                    img_path = parts[1]\n",
    "                    img_id_to_path[img_id] = img_path\n",
    "\n",
    "    # Combine image paths with class labels\n",
    "    for img_id, img_path in img_id_to_path.items():\n",
    "        if img_id in img_to_class:\n",
    "            class_id = img_to_class[img_id]\n",
    "            if class_id in class_id_to_name:\n",
    "                class_name = class_id_to_name[class_id]\n",
    "                class_names.add(class_name)\n",
    "\n",
    "                full_img_path = images_path / img_path\n",
    "                if full_img_path.exists():\n",
    "                    data.append((str(full_img_path), class_name, None))\n",
    "\n",
    "    return data\n",
    "\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "def process_animals10_dataset(animals10_path, class_names):\n",
    "    \"\"\"Process Animals10 dataset\"\"\"\n",
    "    data = []\n",
    "    raw_img_path = animals10_path / \"raw-img\"\n",
    "\n",
    "    # Translation mapping (Italian to English)\n",
    "    translation_map = {\n",
    "        'cane': 'dog',\n",
    "        'cavallo': 'horse',\n",
    "        'elefante': 'elephant',\n",
    "        'farfalla': 'butterfly',\n",
    "        'gallina': 'chicken',\n",
    "        'gatto': 'cat',\n",
    "        'mucca': 'cow',\n",
    "        'pecora': 'sheep',\n",
    "        'ragno': 'spider',\n",
    "        'scoiattolo': 'squirrel'\n",
    "    }\n",
    "\n",
    "    if raw_img_path.exists():\n",
    "        for class_folder in raw_img_path.iterdir():\n",
    "            if class_folder.is_dir():\n",
    "                italian_name = class_folder.name\n",
    "                class_name = translation_map.get(italian_name, italian_name)\n",
    "                class_names.add(class_name)\n",
    "\n",
    "                for img_file in class_folder.glob(\"*.jpeg\"):\n",
    "                    data.append((str(img_file), class_name, None))\n",
    "\n",
    "    return data\n",
    "\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "def process_mix_animals_dataset(mix_path, class_names):\n",
    "    \"\"\"Process 90-mix-animals dataset\"\"\"\n",
    "    data = []\n",
    "    animals_path = mix_path / \"animals\" / \"animals\"\n",
    "\n",
    "    if animals_path.exists():\n",
    "        for class_folder in animals_path.iterdir():\n",
    "            if class_folder.is_dir():\n",
    "                class_name = class_folder.name.replace('-', '_').replace(' ', '_')\n",
    "                class_names.add(class_name)\n",
    "\n",
    "                for img_file in class_folder.glob(\"*.jpg\"):\n",
    "                    data.append((str(img_file), class_name, None))\n",
    "\n",
    "    return data\n",
    "\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "def process_detection_dataset(detection_path, class_names):\n",
    "    \"\"\"Process animals-detection-images dataset (already in YOLO format)\"\"\"\n",
    "    data = []\n",
    "\n",
    "    for split in ['train', 'test']:\n",
    "        split_path = detection_path / split\n",
    "        if split_path.exists():\n",
    "            for class_folder in split_path.iterdir():\n",
    "                if class_folder.is_dir():\n",
    "                    class_name = class_folder.name.replace(' ', '_').replace('-', '_')\n",
    "                    class_names.add(class_name)\n",
    "\n",
    "                    # Process images and labels\n",
    "                    for img_file in class_folder.glob(\"*.jpg\"):\n",
    "                        label_file = class_folder / \"Label\" / (img_file.stem + \".txt\")\n",
    "\n",
    "                        bbox = None\n",
    "                        if label_file.exists():\n",
    "                            try:\n",
    "                                with open(label_file, 'r') as f:\n",
    "                                    line = f.readline().strip()\n",
    "                                    if line:\n",
    "                                        parts = line.split()\n",
    "                                        if len(parts) >= 5:\n",
    "                                            _, x_center, y_center, width, height = map(float, parts[:5])\n",
    "                                            bbox = (x_center, y_center, width, height)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                        data.append((str(img_file), class_name, bbox))\n",
    "\n",
    "    return data\n",
    "\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "def process_aquarium_dataset(aquarium_path, class_names):\n",
    "    \"\"\"Process Aquarium dataset\"\"\"\n",
    "    data = []\n",
    "\n",
    "    # More specific fish classification could be added here\n",
    "    class_name = \"fish\"\n",
    "    class_names.add(class_name)\n",
    "\n",
    "    for split in ['train', 'test', 'valid']:\n",
    "        split_path = aquarium_path / split\n",
    "        if split_path.exists():\n",
    "            for img_file in split_path.glob(\"*.jpg\"):\n",
    "                if not str(img_file.parent).endswith('.ipynb_checkpoints'):\n",
    "                    data.append((str(img_file), class_name, None))\n",
    "\n",
    "    return data\n",
    "\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "def create_dataset_yaml(output_path, class_names, dataset_stats):\n",
    "    \"\"\"Create comprehensive dataset.yaml file for YOLOv11 with all species\"\"\"\n",
    "\n",
    "    yaml_content = {\n",
    "        'path': str(output_path),\n",
    "        'train': 'train/images',\n",
    "        'val': 'val/images',\n",
    "        'test': 'test/images',\n",
    "        'nc': len(class_names),\n",
    "        'names': class_names,  # This contains ALL species as individual classes\n",
    "\n",
    "        # Additional metadata\n",
    "        'dataset_info': {\n",
    "            'description': 'Multi-domain animal detection dataset',\n",
    "            'total_classes': len(class_names),\n",
    "            'total_samples': sum(dataset_stats.values()),\n",
    "            'source_datasets': list(dataset_stats.keys()),\n",
    "            'domains': ['air', 'land', 'sea', 'mixed']\n",
    "        },\n",
    "\n",
    "        # Source dataset breakdown\n",
    "        'source_stats': dataset_stats\n",
    "    }\n",
    "\n",
    "    yaml_path = output_path / 'dataset.yaml'\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(yaml_content, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "    print(f\"üìÑ Created dataset.yaml with {len(class_names)} individual species classes\")\n",
    "\n",
    "    # Also create a detailed class list file\n",
    "    classes_path = output_path / 'classes.txt'\n",
    "    with open(classes_path, 'w') as f:\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            f.write(f\"{i}: {class_name}\\n\")\n",
    "\n",
    "    print(f\"üìù Created classes.txt with detailed class mapping\")\n",
    "\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "def visualize_sample_images(dataset_path, num_samples=9):\n",
    "    \"\"\"Visualize sample images from the prepared dataset\"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    train_images_path = dataset_path / 'train' / 'images'\n",
    "    train_labels_path = dataset_path / 'train' / 'labels'\n",
    "\n",
    "    # Load class names\n",
    "    with open(dataset_path / 'dataset.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    class_names = config['names']\n",
    "\n",
    "    # Get random sample images\n",
    "    image_files = list(train_images_path.glob('*.jpg')) + list(train_images_path.glob('*.jpeg'))\n",
    "    sample_files = random.sample(image_files, min(num_samples, len(image_files)))\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, img_file in enumerate(sample_files):\n",
    "        # Load image\n",
    "        img = cv2.imread(str(img_file))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load corresponding label\n",
    "        label_file = train_labels_path / (img_file.stem + '.txt')\n",
    "        class_id = 0\n",
    "        if label_file.exists():\n",
    "            with open(label_file, 'r') as f:\n",
    "                line = f.readline().strip()\n",
    "                if line:\n",
    "                    class_id = int(line.split()[0])\n",
    "\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Class: {class_names[class_id]}\", fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting YOLOv11 Dataset Preparation\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Prepare the dataset\n",
    "    try:\n",
    "        dataset_path, classes, splits = prepare_yolo_dataset(\n",
    "            base_path=\"/content/data\", # Root folder of all the datasets\n",
    "            output_path=\"/content/yolo_dataset\", # Output path of the organized/prepared data\n",
    "            train_split=0.8,\n",
    "            val_split=0.1,\n",
    "            test_split=0.1,\n",
    "            max_samples_per_class=1000  # Optional: balance dataset\n",
    "        )\n",
    "\n",
    "        print(f\"\\nüéØ Sample classes: {classes[:10]}\")\n",
    "\n",
    "        # Visualize sample images\n",
    "        print(\"\\nüñºÔ∏è  Visualizing sample images...\")\n",
    "        visualize_sample_images(dataset_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        print(\"Make sure your data directory structure matches the expected format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycrkN5yZeP9f"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZ_wD0ZbiiEf"
   },
   "source": [
    "## **Clean classes.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uB9hOoRofo2C"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Path to classes.txt (replaces the uncleaned classes.txt with the cleaned one)\n",
    "in_file = \"/content/yolo_dataset/classes.txt\"\n",
    "out_file = \"/content/yolo_dataset/classes.txt\"\n",
    "\n",
    "# Load the .txt in 'lines'\n",
    "with open(in_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "cleaned = []\n",
    "for line in lines:\n",
    "    # Split on ':' then remove the leading numeric prefix (like 001)\n",
    "    name = line.split(\":\")[1].strip()\n",
    "    name = re.sub(r\"^\\d+\", \"\", name)   # remove starting digits using re\n",
    "    cleaned.append(name)\n",
    "\n",
    "# Write cleaned file\n",
    "with open(out_file, \"w\") as f:\n",
    "    f.write(\"\\n\".join(cleaned))\n",
    "\n",
    "print(f\"‚úÖ Cleaned classes written to: {out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSueWJc2isY5"
   },
   "source": [
    "## **Clean dataset.yaml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajFkckZviM6j"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import re\n",
    "\n",
    "yaml_file = \"/content/yolo_dataset/dataset.yaml\"\n",
    "\n",
    "# Load yaml\n",
    "with open(yaml_file, \"r\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "# Clean names list\n",
    "cleaned_names = [re.sub(r\"^\\d+\", \"\", name) for name in data[\"names\"]]\n",
    "data[\"names\"] = cleaned_names\n",
    "\n",
    "# Save back\n",
    "with open(yaml_file, \"w\") as f:\n",
    "    yaml.dump(data, f, sort_keys=False)\n",
    "\n",
    "print(\"‚úÖ Fixed dataset.yaml. First 10 names:\")\n",
    "print(data[\"names\"][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuCfiouZmdc4"
   },
   "source": [
    "# **Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCRJN9g2QHfP"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv11 model\n",
    "model = YOLO('yolo11m.pt')\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data='/content/yolo_dataset/dataset.yaml',\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoaE2UsaoBqA"
   },
   "source": [
    "# **Evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NaYXZgC0oA6Y"
   },
   "outputs": [],
   "source": [
    "model = YOLO(\"/content/yolo_dataset/runs/detect/train/weights/best.pt\")\n",
    "\n",
    "# Validate on your dataset (this gives precision, recall, mAP, confusion matrix, etc.)\n",
    "metrics = model.val(\n",
    "    data=\"/content/dataset/meta.yaml\",\n",
    "    split=\"test\",\n",
    "    imgsz=512\n",
    ")\n",
    "\n",
    "print(metrics)  # shows metrics dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xU1Wg0-rrP_d"
   },
   "source": [
    "# **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHwuxRW4rPLS"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load your trained model\n",
    "model = YOLO(\"/content/yolo_dataset/runs/detect/train/weights/best.pt\")\n",
    "\n",
    "# Upload an image\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Run inference (object detection)\n",
    "for filename in uploaded.keys():\n",
    "    results = model.predict(\n",
    "        source=filename,\n",
    "        save=True, # save annotated image(s)\n",
    "        conf=0.25, # confidence threshold\n",
    "        imgsz=640\n",
    "    )\n",
    "\n",
    "    print(f\"Processed {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2gUtVMTmZxs"
   },
   "source": [
    "### **Optional Step - Exporting the Trained Model (for Deployment & Portability)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drmVXLIZi7Je"
   },
   "outputs": [],
   "source": [
    "# PyTorch format (.pt)\n",
    "# Export converts it to other formats:\n",
    "# ONNX ‚Üí open format, runs on many platforms (good for production).\n",
    "# TorchScript ‚Üí optimized PyTorch model (faster inference).\n",
    "# TensorRT / CoreML ‚Üí for NVIDIA GPUs / Apple devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDmdJfwPm8BI"
   },
   "outputs": [],
   "source": [
    "# IoU (Intersection over union) = how much the modal drawn box overlapped with the real box (ground truth).\n",
    "# mAP = your average score across all objects and overlap thresholds.\n",
    "# Export = translating your model into another ‚Äúlanguage‚Äù so other systems can use it."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOCVeVcWn888VYYJO0AJ5eH",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
